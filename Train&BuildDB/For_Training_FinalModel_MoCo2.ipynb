{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1583,"status":"ok","timestamp":1704023911195,"user":{"displayName":"이지환","userId":"14330397087404565328"},"user_tz":-540},"id":"lG51uMX7uIut","outputId":"69cd3a2f-7429-485a-805f-a1b9850add6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Package                          Version\n","-------------------------------- ---------------------\n","absl-py                          1.4.0\n","aiohttp                          3.9.1\n","aiosignal                        1.3.1\n","alabaster                        0.7.13\n","albumentations                   1.3.1\n","altair                           4.2.2\n","anyio                            3.7.1\n","appdirs                          1.4.4\n","argon2-cffi                      23.1.0\n","argon2-cffi-bindings             21.2.0\n","array-record                     0.5.0\n","arviz                            0.15.1\n","astropy                          5.3.4\n","astunparse                       1.6.3\n","async-timeout                    4.0.3\n","atpublic                         4.0\n","attrs                            23.1.0\n","audioread                        3.0.1\n","autograd                         1.6.2\n","Babel                            2.14.0\n","backcall                         0.2.0\n","beautifulsoup4                   4.11.2\n","bidict                           0.22.1\n","bigframes                        0.17.0\n","bleach                           6.1.0\n","blinker                          1.4\n","blis                             0.7.11\n","blosc2                           2.0.0\n","bokeh                            3.3.2\n","bqplot                           0.12.42\n","branca                           0.7.0\n","build                            1.0.3\n","CacheControl                     0.13.1\n","cachetools                       5.3.2\n","catalogue                        2.0.10\n","certifi                          2023.11.17\n","cffi                             1.16.0\n","chardet                          5.2.0\n","charset-normalizer               3.3.2\n","chex                             0.1.7\n","click                            8.1.7\n","click-plugins                    1.1.1\n","cligj                            0.7.2\n","cloudpickle                      2.2.1\n","cmake                            3.27.9\n","cmdstanpy                        1.2.0\n","colorcet                         3.0.1\n","colorlover                       0.3.0\n","colour                           0.1.5\n","community                        1.0.0b1\n","confection                       0.1.4\n","cons                             0.4.6\n","contextlib2                      21.6.0\n","contourpy                        1.2.0\n","cryptography                     41.0.7\n","cufflinks                        0.17.3\n","cupy-cuda12x                     12.2.0\n","cvxopt                           1.3.2\n","cvxpy                            1.3.2\n","cycler                           0.12.1\n","cymem                            2.0.8\n","Cython                           3.0.6\n","dask                             2023.8.1\n","datascience                      0.17.6\n","db-dtypes                        1.2.0\n","dbus-python                      1.2.18\n","debugpy                          1.6.6\n","decorator                        4.4.2\n","defusedxml                       0.7.1\n","diskcache                        5.6.3\n","distributed                      2023.8.1\n","distro                           1.7.0\n","dlib                             19.24.2\n","dm-tree                          0.1.8\n","docutils                         0.18.1\n","dopamine-rl                      4.0.6\n","duckdb                           0.9.2\n","earthengine-api                  0.1.384\n","easydict                         1.11\n","ecos                             2.0.12\n","editdistance                     0.6.2\n","eerepr                           0.0.4\n","efficientnet-pytorch             0.7.1\n","en-core-web-sm                   3.6.0\n","entrypoints                      0.4\n","et-xmlfile                       1.1.0\n","etils                            1.6.0\n","etuples                          0.3.9\n","exceptiongroup                   1.2.0\n","fastai                           2.7.13\n","fastcore                         1.5.29\n","fastdownload                     0.0.7\n","fastjsonschema                   2.19.0\n","fastprogress                     1.0.3\n","fastrlock                        0.8.2\n","filelock                         3.13.1\n","fiona                            1.9.5\n","firebase-admin                   5.3.0\n","Flask                            2.2.5\n","flatbuffers                      23.5.26\n","flax                             0.7.5\n","folium                           0.14.0\n","fonttools                        4.46.0\n","frozendict                       2.3.10\n","frozenlist                       1.4.1\n","fsspec                           2023.6.0\n","future                           0.18.3\n","gast                             0.5.4\n","gcsfs                            2023.6.0\n","GDAL                             3.4.3\n","gdown                            4.6.6\n","geemap                           0.29.6\n","gensim                           4.3.2\n","geocoder                         1.38.1\n","geographiclib                    2.0\n","geopandas                        0.13.2\n","geopy                            2.3.0\n","gin-config                       0.5.0\n","glob2                            0.7\n","google                           2.0.3\n","google-ai-generativelanguage     0.4.0\n","google-api-core                  2.11.1\n","google-api-python-client         2.84.0\n","google-auth                      2.17.3\n","google-auth-httplib2             0.1.1\n","google-auth-oauthlib             1.2.0\n","google-cloud-aiplatform          1.38.1\n","google-cloud-bigquery            3.12.0\n","google-cloud-bigquery-connection 1.12.1\n","google-cloud-bigquery-storage    2.24.0\n","google-cloud-core                2.3.3\n","google-cloud-datastore           2.15.2\n","google-cloud-firestore           2.11.1\n","google-cloud-functions           1.13.3\n","google-cloud-iam                 2.13.0\n","google-cloud-language            2.9.1\n","google-cloud-resource-manager    1.11.0\n","google-cloud-storage             2.8.0\n","google-cloud-translate           3.11.3\n","google-colab                     1.0.0\n","google-crc32c                    1.5.0\n","google-generativeai              0.3.1\n","google-pasta                     0.2.0\n","google-resumable-media           2.7.0\n","googleapis-common-protos         1.62.0\n","googledrivedownloader            0.4\n","graphviz                         0.20.1\n","greenlet                         3.0.2\n","grpc-google-iam-v1               0.13.0\n","grpcio                           1.60.0\n","grpcio-status                    1.48.2\n","gspread                          3.4.2\n","gspread-dataframe                3.3.1\n","gym                              0.25.2\n","gym-notices                      0.0.8\n","h5netcdf                         1.3.0\n","h5py                             3.9.0\n","holidays                         0.38\n","holoviews                        1.17.1\n","html5lib                         1.1\n","httpimport                       1.3.1\n","httplib2                         0.22.0\n","huggingface-hub                  0.19.4\n","humanize                         4.7.0\n","hyperopt                         0.2.7\n","ibis-framework                   6.2.0\n","idna                             3.6\n","imageio                          2.31.6\n","imageio-ffmpeg                   0.4.9\n","imagesize                        1.4.1\n","imbalanced-learn                 0.10.1\n","imgaug                           0.4.0\n","importlib-metadata               7.0.0\n","importlib-resources              6.1.1\n","imutils                          0.5.4\n","inflect                          7.0.0\n","iniconfig                        2.0.0\n","install                          1.3.5\n","intel-openmp                     2023.2.3\n","ipyevents                        2.0.2\n","ipyfilechooser                   0.6.0\n","ipykernel                        5.5.6\n","ipyleaflet                       0.18.1\n","ipython                          7.34.0\n","ipython-genutils                 0.2.0\n","ipython-sql                      0.5.0\n","ipytree                          0.2.2\n","ipywidgets                       7.7.1\n","itsdangerous                     2.1.2\n","jax                              0.4.23\n","jaxlib                           0.4.23+cuda12.cudnn89\n","jeepney                          0.7.1\n","jieba                            0.42.1\n","Jinja2                           3.1.2\n","joblib                           1.3.2\n","jsonpickle                       3.0.2\n","jsonschema                       4.19.2\n","jsonschema-specifications        2023.11.2\n","jupyter-client                   6.1.12\n","jupyter-console                  6.1.0\n","jupyter_core                     5.5.1\n","jupyter-server                   1.24.0\n","jupyterlab_pygments              0.3.0\n","jupyterlab-widgets               3.0.9\n","kaggle                           1.5.16\n","kagglehub                        0.1.4\n","keras                            2.15.0\n","keyring                          23.5.0\n","kiwisolver                       1.4.5\n","langcodes                        3.3.0\n","launchpadlib                     1.10.16\n","lazr.restfulclient               0.14.4\n","lazr.uri                         1.0.6\n","lazy_loader                      0.3\n","libclang                         16.0.6\n","librosa                          0.10.1\n","lida                             0.0.10\n","lightgbm                         4.1.0\n","linkify-it-py                    2.0.2\n","llmx                             0.0.15a0\n","llvmlite                         0.41.1\n","locket                           1.0.0\n","logical-unification              0.4.6\n","lxml                             4.9.3\n","malloy                           2023.1067\n","Markdown                         3.5.1\n","markdown-it-py                   3.0.0\n","MarkupSafe                       2.1.3\n","matplotlib                       3.7.1\n","matplotlib-inline                0.1.6\n","matplotlib-venn                  0.11.9\n","mdit-py-plugins                  0.4.0\n","mdurl                            0.1.2\n","miniKanren                       1.0.3\n","missingno                        0.5.2\n","mistune                          0.8.4\n","mizani                           0.9.3\n","mkl                              2023.2.0\n","ml-dtypes                        0.2.0\n","mlxtend                          0.22.0\n","more-itertools                   10.1.0\n","moviepy                          1.0.3\n","mpmath                           1.3.0\n","msgpack                          1.0.7\n","multidict                        6.0.4\n","multipledispatch                 1.0.0\n","multitasking                     0.0.11\n","murmurhash                       1.0.10\n","music21                          9.1.0\n","natsort                          8.4.0\n","nbclassic                        1.0.0\n","nbclient                         0.9.0\n","nbconvert                        6.5.4\n","nbformat                         5.9.2\n","nest-asyncio                     1.5.8\n","networkx                         3.2.1\n","nibabel                          4.0.2\n","nltk                             3.8.1\n","notebook                         6.5.5\n","notebook_shim                    0.2.3\n","numba                            0.58.1\n","numexpr                          2.8.8\n","numpy                            1.23.5\n","oauth2client                     4.1.3\n","oauthlib                         3.2.2\n","opencv-contrib-python            4.8.0.76\n","opencv-python                    4.8.0.76\n","opencv-python-headless           4.8.1.78\n","openpyxl                         3.1.2\n","opt-einsum                       3.3.0\n","optax                            0.1.7\n","orbax-checkpoint                 0.4.4\n","osqp                             0.6.2.post8\n","packaging                        23.2\n","pandas                           1.5.3\n","pandas-datareader                0.10.0\n","pandas-gbq                       0.19.2\n","pandas-stubs                     1.5.3.230304\n","pandocfilters                    1.5.0\n","panel                            1.3.4\n","param                            2.0.1\n","parso                            0.8.3\n","parsy                            2.1\n","partd                            1.4.1\n","pathlib                          1.0.1\n","pathy                            0.10.3\n","patsy                            0.5.4\n","peewee                           3.17.0\n","pexpect                          4.9.0\n","pickleshare                      0.7.5\n","Pillow                           9.4.0\n","pip                              23.1.2\n","pip-tools                        6.13.0\n","platformdirs                     4.1.0\n","plotly                           5.15.0\n","plotnine                         0.12.4\n","pluggy                           1.3.0\n","polars                           0.17.3\n","pooch                            1.8.0\n","portpicker                       1.5.2\n","prefetch-generator               1.0.3\n","preshed                          3.0.9\n","prettytable                      3.9.0\n","proglog                          0.1.10\n","progressbar2                     4.2.0\n","prometheus-client                0.19.0\n","promise                          2.3\n","prompt-toolkit                   3.0.43\n","prophet                          1.1.5\n","proto-plus                       1.23.0\n","protobuf                         3.20.3\n","psutil                           5.9.5\n","psycopg2                         2.9.9\n","ptyprocess                       0.7.0\n","py-cpuinfo                       9.0.0\n","py4j                             0.10.9.7\n","pyarrow                          10.0.1\n","pyasn1                           0.5.1\n","pyasn1-modules                   0.3.0\n","pycocotools                      2.0.7\n","pycparser                        2.21\n","pyct                             0.5.0\n","pydantic                         1.10.13\n","pydata-google-auth               1.8.2\n","pydot                            1.4.2\n","pydot-ng                         2.0.0\n","pydotplus                        2.0.2\n","PyDrive                          1.3.1\n","PyDrive2                         1.6.3\n","pyerfa                           2.0.1.1\n","pygame                           2.5.2\n","Pygments                         2.16.1\n","PyGObject                        3.42.1\n","PyJWT                            2.3.0\n","pymc                             5.7.2\n","pymystem3                        0.2.0\n","PyOpenGL                         3.1.7\n","pyOpenSSL                        23.3.0\n","pyparsing                        3.1.1\n","pyperclip                        1.8.2\n","pyproj                           3.6.1\n","pyproject_hooks                  1.0.0\n","pyshp                            2.3.1\n","PySocks                          1.7.1\n","pytensor                         2.14.2\n","pytest                           7.4.3\n","python-apt                       0.0.0\n","python-box                       7.1.1\n","python-dateutil                  2.8.2\n","python-louvain                   0.16\n","python-slugify                   8.0.1\n","python-utils                     3.8.1\n","pytz                             2023.3.post1\n","pyviz_comms                      3.0.0\n","PyWavelets                       1.5.0\n","PyYAML                           6.0.1\n","pyzmq                            23.2.1\n","qdldl                            0.1.7.post0\n","qudida                           0.0.4\n","ratelim                          0.1.6\n","referencing                      0.32.0\n","regex                            2023.6.3\n","requests                         2.31.0\n","requests-oauthlib                1.3.1\n","requirements-parser              0.5.0\n","rich                             13.7.0\n","rpds-py                          0.15.2\n","rpy2                             3.4.2\n","rsa                              4.9\n","safetensors                      0.4.1\n","scikit-image                     0.19.3\n","scikit-learn                     1.2.2\n","scipy                            1.11.4\n","scooby                           0.9.2\n","scs                              3.2.4.post1\n","seaborn                          0.12.2\n","SecretStorage                    3.3.1\n","Send2Trash                       1.8.2\n","setuptools                       67.7.2\n","shapely                          2.0.2\n","six                              1.16.0\n","sklearn-pandas                   2.2.0\n","smart-open                       6.4.0\n","sniffio                          1.3.0\n","snowballstemmer                  2.2.0\n","sortedcontainers                 2.4.0\n","soundfile                        0.12.1\n","soupsieve                        2.5\n","soxr                             0.3.7\n","spacy                            3.6.1\n","spacy-legacy                     3.0.12\n","spacy-loggers                    1.0.5\n","Sphinx                           5.0.2\n","sphinxcontrib-applehelp          1.0.7\n","sphinxcontrib-devhelp            1.0.5\n","sphinxcontrib-htmlhelp           2.0.4\n","sphinxcontrib-jsmath             1.0.1\n","sphinxcontrib-qthelp             1.0.6\n","sphinxcontrib-serializinghtml    1.1.9\n","SQLAlchemy                       2.0.23\n","sqlglot                          17.16.2\n","sqlparse                         0.4.4\n","srsly                            2.4.8\n","stanio                           0.3.0\n","statsmodels                      0.14.1\n","sympy                            1.12\n","tables                           3.8.0\n","tabulate                         0.9.0\n","tbb                              2021.11.0\n","tblib                            3.0.0\n","tenacity                         8.2.3\n","tensorboard                      2.15.1\n","tensorboard-data-server          0.7.2\n","tensorflow                       2.15.0\n","tensorflow-datasets              4.9.4\n","tensorflow-estimator             2.15.0\n","tensorflow-gcs-config            2.15.0\n","tensorflow-hub                   0.15.0\n","tensorflow-io-gcs-filesystem     0.34.0\n","tensorflow-metadata              1.14.0\n","tensorflow-probability           0.22.0\n","tensorstore                      0.1.45\n","termcolor                        2.4.0\n","terminado                        0.18.0\n","text-unidecode                   1.3\n","textblob                         0.17.1\n","tf-slim                          1.1.0\n","thinc                            8.1.12\n","threadpoolctl                    3.2.0\n","tifffile                         2023.12.9\n","tinycss2                         1.2.1\n","tokenizers                       0.15.0\n","toml                             0.10.2\n","tomli                            2.0.1\n","toolz                            0.12.0\n","torch                            2.1.0+cu121\n","torchaudio                       2.1.0+cu121\n","torchdata                        0.7.0\n","torchsummary                     1.5.1\n","torchtext                        0.16.0\n","torchvision                      0.16.0+cu121\n","tornado                          6.3.2\n","tqdm                             4.66.1\n","traitlets                        5.7.1\n","traittypes                       0.2.1\n","transformers                     4.35.2\n","triton                           2.1.0\n","tweepy                           4.14.0\n","typer                            0.9.0\n","types-pytz                       2023.3.1.1\n","types-setuptools                 69.0.0.0\n","typing_extensions                4.5.0\n","tzlocal                          5.2\n","uc-micro-py                      1.0.2\n","uritemplate                      4.1.1\n","urllib3                          2.0.7\n","vega-datasets                    0.9.0\n","wadllib                          1.3.6\n","wasabi                           1.1.2\n","wcwidth                          0.2.12\n","webcolors                        1.13\n","webencodings                     0.5.1\n","websocket-client                 1.7.0\n","Werkzeug                         3.0.1\n","wheel                            0.42.0\n","widgetsnbextension               3.6.6\n","wordcloud                        1.9.3\n","wrapt                            1.14.1\n","xarray                           2023.7.0\n","xarray-einstats                  0.6.0\n","xgboost                          2.0.2\n","xlrd                             2.0.1\n","xxhash                           3.4.1\n","xyzservices                      2023.10.1\n","yarl                             1.9.4\n","yellowbrick                      1.5\n","yfinance                         0.2.33\n","zict                             3.0.0\n","zipp                             3.17.0\n"]}],"source":["!pip list"]},{"cell_type":"markdown","metadata":{"id":"IStwLD44r5ht"},"source":["# Drive Mount & Install EfficientNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19538,"status":"ok","timestamp":1702709759992,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"fkKSFL3vr5EA","outputId":"4c7189b2-7ef3-4c9e-8e68-1aa46a58f001"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6970,"status":"ok","timestamp":1704023909617,"user":{"displayName":"이지환","userId":"14330397087404565328"},"user_tz":-540},"id":"fdGs6SxFsBxx","outputId":"1ef39add-b55e-4c31-e72f-1e06bd5b3429"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting efficientnet-pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=2aa38946258a7556cf52042f50328f7f84c236f69a6a32a27006c92daa40736b\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.1\n"]}],"source":["!pip install efficientnet-pytorch"]},{"cell_type":"markdown","metadata":{"id":"EZwSzKmWrunv"},"source":["# Zip Extract & corrupted file 제거\n","- 중복은 제거함"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-2EQFLdr345"},"outputs":[],"source":["import zipfile\n","\n","with zipfile.ZipFile('/content/drive/MyDrive/Poster.zip') as zip_ref:\n","    zip_ref.extractall()\n","\n","# with zipfile.ZipFile('/content/drive/MyDrive/Data/Poster/Book.zip') as zip_ref:\n","#     zip_ref.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-_jNbEkr31i"},"outputs":[],"source":["import PIL\n","from PIL import Image\n","import os\n","from tqdm.auto import tqdm\n","\n","# 이미지가 저장된 디렉토리의 경로를 설정합니다.\n","image_directories = ['/content/Poster/Concert','/content/Poster/Contest','/content/Poster/Movie', '/content/Poster/Book']\n","\n","# 손상된 파일의 목록을 저장할 리스트\n","corrupted_files = []\n","\n","for image_directory in tqdm(image_directories):\n","\n","    # 디렉토리 내의 모든 파일을 확인합니다.\n","    for filename in tqdm(os.listdir(image_directory)):\n","        # 파일 경로를 구성합니다.\n","        file_path = os.path.join(image_directory, filename)\n","        try:\n","            # 이미지 파일을 엽니다. 오류가 발생하면 except 블록으로 이동합니다.\n","            with Image.open(file_path) as img:\n","                # 파일이 이미지로 열리는지 간단하게 확인합니다.\n","                img.verify()\n","        except Exception as e:\n","            # 오류가 발생한 파일을 리스트에 추가합니다.\n","            corrupted_files.append(file_path)\n","\n","    # 손상된 파일들을 제거하거나 다른 처리를 수행합니다.\n","for corrupted_file in corrupted_files:\n","    os.remove(corrupted_file)\n","\n","print(f'Removed corrupted files')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luvd93lHfDtz"},"outputs":[],"source":["# save the corrupted_files\n","import pandas as pd\n","df = pd.DataFrame(corrupted_files)\n","df.columns = ['corrupted_files']\n","df.to_csv('/content/drive/MyDrive/final/corrupted_files.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1540,"status":"ok","timestamp":1702710902301,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"Te0WIbnadxDu","outputId":"516b4efa-f378-4f12-9361-755388b5aba1"},"outputs":[{"name":"stdout","output_type":"stream","text":["All corrupted files removed.\n"]}],"source":["import pandas as pd\n","import os\n","\n","# CSV 파일에서 손상된 파일 목록 불러오기\n","corrupted_files_df = pd.read_csv('/content/drive/MyDrive/final/corrupted_files.csv')\n","\n","# 파일 경로를 리스트로 변환\n","corrupted_files_list = corrupted_files_df['corrupted_files'].tolist()\n","\n","# 손상된 파일들 삭제\n","for file_path in corrupted_files_list:\n","    # 파일이 존재하는지 확인 후 삭제\n","    if os.path.exists(file_path):\n","        os.remove(file_path)\n","        # print(f'Removed: {file_path}')\n","    else:\n","        print(f'File not found: {file_path}')\n","\n","print('All corrupted files removed.')"]},{"cell_type":"markdown","metadata":{"id":"NYnGZzVpyqgu"},"source":["# Calculate Mean&Std of our Dataset"]},{"cell_type":"markdown","metadata":{"id":"k2HxQoQtz8-6"},"source":["## load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jy7o_tn0hWli"},"outputs":[],"source":["# Custom dataset for MoCov2\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset\n","import torchvision.transforms as transforms\n","class CustomDataset(Dataset):\n","    def __init__(self, folder_path, transform=None):\n","        self.folder_path = folder_path\n","        self.image_list = os.listdir(folder_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        image_path = os.path.join(self.folder_path, self.image_list[idx])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image\n","\n","norm_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset_Concert = CustomDataset(folder_path='/content/Poster/Concert', transform=norm_transform)\n","train_dataset_Contest = CustomDataset(folder_path='/content/Poster/Contest', transform=norm_transform)\n","train_dataset_Movie = CustomDataset(folder_path='/content/Poster/Movie', transform=norm_transform)\n","train_dataset_Book = CustomDataset(folder_path='/content/Poster/Book', transform=norm_transform)\n","train_dataset = ConcatDataset([train_dataset_Concert,train_dataset_Contest,train_dataset_Movie,train_dataset_Book])"]},{"cell_type":"markdown","metadata":{"id":"6ElBgBH4z3BE"},"source":["## Calculate Mean&Std\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["aad1e6422bb74de8abf29b4b24b470e5","cbfe5b25047240b1a81163f5c4012b7c","d26cd483ecfa43dbacc04cebeb661ded","a260deee41fa46038a3bcd4004958dd1","59a031273ad648a1842b4265b77765cd","50c2f42459ca4ce39b8b2854756c3897","14c00c02187d49b28025486bd5bc0d25","02c08f63b86b441f9fb8253f5eb155fa","10451da80eb0424c8194ebfaec4b089d","ff38d357f94046d896c0ce0c941bae8a","f98509c883b74804964a44b0db13faf5"]},"executionInfo":{"elapsed":3696526,"status":"ok","timestamp":1702632488135,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"pFRU0jWPypfx","outputId":"63abfef9-df80-4ccc-ce97-dce0861fbd07"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aad1e6422bb74de8abf29b4b24b470e5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/275965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["평균(R,G,B): [0.62706534 0.6120791  0.59086932]\n","표준편차(R,G,B): [0.37403608 0.35793398 0.36176297]\n"]}],"source":["import numpy as np\n","from tqdm.auto import tqdm\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","Image.MAX_IMAGE_PIXELS = None\n","\n","def calculate_norm_optimized(dataset):\n","    # RGB 채널을 위한 리스트 초기화\n","    channel_sums = np.zeros(3)\n","    channel_squares = np.zeros(3)\n","    num_pixels = 0\n","\n","    # 데이터셋 순회\n","    for x in tqdm(dataset):\n","        x_np = x.numpy()\n","        # RGB 채널 별 평균을 계산하기 위해 합계 갱신\n","        channel_sums += np.sum(x_np, axis=(1, 2))\n","        # 제곱의 합계를 계산하여 표준편차 계산을 위해 사용\n","        channel_squares += np.sum(x_np**2, axis=(1, 2))\n","        # 총 픽셀 수 업데이트\n","        num_pixels += x_np.shape[1] * x_np.shape[2]\n","\n","    # RGB 채널별 평균\n","    mean = channel_sums / num_pixels\n","\n","    # RGB 채널별 표준편차\n","    std = np.sqrt(channel_squares / num_pixels - mean**2)\n","\n","    return mean, std\n","\n","mean_, std_ = calculate_norm_optimized(train_dataset)\n","print(f'평균(R,G,B): {mean_}\\n표준편차(R,G,B): {std_}')"]},{"cell_type":"markdown","metadata":{"id":"so_RKf-m0AbT"},"source":["## save Mean&Std"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702632488136,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"cpBjlHAhzA7d","outputId":"a56f6c88-1a9b-40e1-c2bb-a1d9a117f77a"},"outputs":[{"name":"stdout","output_type":"stream","text":["평균 및 표준편차가 /content/drive/MyDrive/final/dataset_mean_std.pkl에 저장됨\n"]}],"source":["# 평균&표준편차 저장\n","import pickle\n","\n","# 저장할 파일의 경로를 지정\n","file_path = '/content/drive/MyDrive/final/dataset_mean_std.pkl'\n","\n","# mean_와 std_를 파일에 저장\n","with open(file_path, 'wb') as f:\n","    pickle.dump((mean_, std_), f)\n","\n","print(f'평균 및 표준편차가 {file_path}에 저장됨')"]},{"cell_type":"markdown","metadata":{"id":"QJXQHWD10D3j"},"source":["## load Mean&Std"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXD-MK2kiJqp"},"outputs":[],"source":["file_path = '/content/drive/MyDrive/final/dataset_mean_std.pkl'\n","\n","with open(file_path, 'rb') as f:\n","    mean_, std_ = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"86lf8Dp2tR11"},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{"id":"-lIyxMslrvAx"},"source":["## Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1215,"status":"ok","timestamp":1702632705404,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"AT3Z542tri-r","outputId":"bd36f884-e7dd-48b0-ba8f-47949855a6af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b2\n"]}],"source":["# EfficientNet-PyTorch (참조: \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\", Tan and Le, 2019)\n","\n","# 아래는 MoCo v2에 대한 구현입니다. MoCo는 \"Momentum Contrast for Unsupervised Visual Representation Learning\" (He et al., 2020)에서 처음 소개되었습니다.\n","# MoCo v2는 \"Improved Baselines with Momentum Contrastive Learning\" (Chen et al., 2020)에서 소개되었습니다.\n","# 이 구현은 원 논문의 접근 방식을 기반으로 하며, 몇 가지 수정사항이 포함되어 있습니다.\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset\n","from torchvision import models, datasets\n","import os\n","from tqdm.auto import tqdm\n","from PIL import Image, ImageFile\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from efficientnet_pytorch import EfficientNet\n","import csv\n","import pandas as pd\n","import copy\n","import psutil\n","\n","# epoch 및 hyperparameter 정의\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","Image.MAX_IMAGE_PIXELS = None\n","num_epochs = 50\n","learning_rate = 3e-4 # = initial learning rate for warm up\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","projection_dim = 128\n","temperature = 0.07\n","momentum = 0.99\n","weight_decay = 1e-5\n","batch_size = 192\n","queue_size = 384\n","dropout_rate = 0.5\n","\n","# Define the MoCov2 model\n","class MoCov2Model(nn.Module):\n","    def __init__(self, base_encoder, projection_dim=128, temperature=0.07, dropout_rate=0.5, queue_size=384, momentum=0.99):\n","        super(MoCov2Model, self).__init__()\n","\n","        # 이미지 특징 추출 - 학습 & 추론 단계에서 모두 사용\n","        # EfficientNet + 128차원의 출력을 가진 선형 레이어 추가하여 DB에 저장할 특징 벡터의 차원 수 축소\n","        self.encoder = nn.Sequential(\n","            base_encoder,\n","            nn.Linear(1000, 128)\n","        )\n","\n","        # Encoder에 의해 추출된 특징을 projection - 학습 과정에서 contrastive loss를 계산하는 데 사용, 추론 시엔 사용 X\n","        # Projection head 추가\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(128, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, projection_dim),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        # 모멘텀 인코더의 복사본 생성 (깊은 복사)\n","        self.momentum_encoder = copy.deepcopy(self.encoder)\n","        self.momentum_projection_head = copy.deepcopy(self.projection_head)\n","\n","        # 모멘텀 인코더와 projection head의 모든 파라미터를 고정 (훈련 중 업데이트 방지)\n","        for param in self.momentum_encoder.parameters():\n","            param.requires_grad = False\n","        for param in self.momentum_projection_head.parameters():\n","            param.requires_grad = False\n","\n","        # 기타 초기화\n","        self.queue = torch.zeros(queue_size, projection_dim).to(device)\n","        self.queue_ptr = 0\n","        self.momentum = momentum\n","\n","    def _momentum_update(self):\n","        # 모멘텀 인코더 및 projection head의 가중치를 업데이트하는 함수\n","        with torch.no_grad():\n","            for param_q, param_k in zip(self.encoder.parameters(), self.momentum_encoder.parameters()):\n","                param_k.data = param_k.data * self.momentum + param_q.data * (1. - self.momentum)\n","            for param_q, param_k in zip(self.projection_head.parameters(), self.momentum_projection_head.parameters()):\n","                param_k.data = param_k.data * self.momentum + param_q.data * (1. - self.momentum)\n","\n","    def forward(self, x, with_projection_head=True):\n","        x = self.encoder(x)\n","        if with_projection_head:\n","            x = self.projection_head(x)\n","        return x\n","\n","    def enqueue_dequeue(self, keys):\n","        # 큐에 새로운 데이터 추가 및 오래된 데이터 제거 (CPU에서 수행)\n","        keys = keys.to('cpu')  # GPU에서 CPU로 이동\n","        batch_size = keys.size(0)\n","        ptr = int(self.queue_ptr)\n","\n","        # replace the keys at ptr (dequeue and enqueue)\n","        space_left = self.queue.size(0) - ptr  # 남은 공간 계산\n","        # 마지막 배치의 크기가 다른 배치보다 작을 때 발생할 수 있는 예외 상황 처리\n","        if space_left < batch_size:\n","            # If 큐의 남은 공간이 batch_size보다 작을 경우, split the update\n","            self.queue[ptr:] = keys[:space_left]\n","            self.queue[:batch_size - space_left] = keys[space_left:]\n","            ptr = batch_size - space_left\n","        else:\n","            self.queue[ptr:ptr + batch_size] = keys\n","            ptr = (ptr + batch_size) % self.queue.size(0)  # move pointer\n","\n","        self.queue_ptr = ptr\n","\n","# 모델이 유사한 이미지를 서로 가깝게, 그리고 서로 다른 이미지를 멀게 배치하도록 학습\n","# MoCov2Loss(InfoNCE Loss)\n","class MoCov2Loss(nn.Module):\n","    def __init__(self, temperature=0.07):\n","        super(MoCov2Loss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j, queue):\n","        batch_size = z_i.size(0)\n","\n","        # 특징 벡터 정규화\n","        z_i_norm = nn.functional.normalize(z_i, dim=1)\n","        z_j_norm = nn.functional.normalize(z_j, dim=1)\n","        queue_norm = nn.functional.normalize(queue, dim=1)\n","\n","        # Positive 쌍에 대한 유사도 계산\n","        positive_sim = torch.exp(torch.sum(z_i_norm * z_j_norm, dim=1) / self.temperature)\n","\n","        # Negative 쌍에 대한 유사도 계산\n","        negative_sim = torch.exp(torch.mm(z_i_norm, queue_norm.t()) / self.temperature)\n","        negative_sim_sum = torch.sum(negative_sim, dim=1)\n","\n","        # 손실 계산\n","        loss = -torch.log(positive_sim / (positive_sim + negative_sim_sum)).mean()\n","\n","        return loss\n","\n","# Custom dataset for MoCov2\n","class CustomDataset(Dataset):\n","    def __init__(self, folder_path, transform=None):\n","        self.folder_path = folder_path\n","        self.image_list = os.listdir(folder_path)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        image_path = os.path.join(self.folder_path, self.image_list[idx])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image_i = self.transform(image)\n","            image_j = self.transform(image)\n","        else:\n","            image_i = image\n","            image_j = image\n","\n","        return image_i, image_j\n","\n","# Early Stop to avoid overfitting\n","# Early stopping의 개념은 \"Early Stopping - But When?\" (Lutz Prechelt, 1998)에서 자세히 설명되어 있습니다.\n","class EarlyStopping:\n","    def __init__(self, patience=4, verbose=False, delta=0, path='checkpoint.pt'):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.delta = delta\n","        self.path = path\n","        self.best_score = None\n","        self.early_stop = False\n","        self.counter = 0\n","\n","    def __call__(self, epoch_loss, model):\n","        score = -epoch_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(epoch_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            if self.verbose:\n","                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(epoch_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, epoch_loss, model):\n","        '''Saves model when the validation loss decrease.'''\n","        if self.verbose:\n","            print(f'Training loss decreased ({self.best_score:.6f} --> {epoch_loss:.6f}). Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","\n","# Set up the data loader for training\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(size=224, scale=(0.6, 1.)),\n","    transforms.ColorJitter(0.5, 0.5, 0.5, 0.5),  # 밝기, 대비, 채도, 색조 조절\n","    transforms.RandomGrayscale(p=0.3),\n","    transforms.RandomApply([transforms.GaussianBlur(kernel_size=9)], p=0.5),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean_, std_), # 수집한 데이터들의 RGB 픽셀 평균 & 표준편차 적용\n","])\n","\n","# EfficientNet 모델을 로드하고 MoCov2 모델에 통합\n","# EfficientNet 관련 참조는 상단에 기술함\n","efficientnet_b2 = EfficientNet.from_pretrained('efficientnet-b2').to(device)\n","mocov2_model = MoCov2Model(efficientnet_b2, projection_dim=projection_dim, temperature=temperature,\n","                           dropout_rate=dropout_rate, queue_size=queue_size, momentum=momentum).to(device)\n","\n","# Set up the data loader for training\n","# train_dataset = CustomDataset(folder_path='/content/Poster/ConcertPoster', transform=train_transform)\n","train_dataset_Concert = CustomDataset(folder_path='/content/Poster/Concert', transform=train_transform)\n","train_dataset_Contest = CustomDataset(folder_path='/content/Poster/Contest', transform=train_transform)\n","train_dataset_Movie = CustomDataset(folder_path='/content/Poster/Movie', transform=train_transform)\n","train_dataset_Book = CustomDataset(folder_path='/content/Poster/Book', transform=train_transform)\n","train_dataset = ConcatDataset([train_dataset_Concert,train_dataset_Contest,train_dataset_Movie,train_dataset_Book])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","\n","# Set up the optimizer\n","optimizer = optim.SGD(mocov2_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n","\n","# Set up the MoCov2 loss function\n","criterion = MoCov2Loss(temperature = temperature)"]},{"cell_type":"markdown","metadata":{"id":"bqozoza5tnvI"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS_H7NXitoll"},"outputs":[],"source":["# 메모리 사용량 체크 함수, 90% 넘어가면 종료\n","def check_memory_usage(threshold):\n","    memory_usage = psutil.virtual_memory().percent\n","    return memory_usage > threshold\n","\n","# 웜업 및 코사인 감소 스케줄링 함수 정의\n","def adjust_learning_rate(optimizer, epoch, initial_lr, num_epochs, warmup_epochs):\n","    \"\"\"\n","        cosine decay schedule에 따라 에포크별 학습률 조정 (with warm-up)\n","    \"\"\"\n","    if epoch < warmup_epochs:\n","        lr = initial_lr * (epoch + 1) / warmup_epochs\n","    else:\n","        lr = 0.5 * initial_lr * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    return lr\n","\n","# 초기 학습률 및 웜업 에포크 설정\n","initial_lr = learning_rate  # 기존에 정의된 learning_rate 사용\n","warmup_epochs = 4  # 웜업 에포크 수\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","Image.MAX_IMAGE_PIXELS = None\n","\n","os.makedirs('/content/drive/MyDrive/final', exist_ok=True)\n","early_stopping = EarlyStopping(patience=4, verbose=True, path='/content/drive/MyDrive/final/mocov2_best_model_231216.pth') # best model\n","\n","train_losses = []\n","\n","for epoch in range(num_epochs):\n","    mocov2_model.train()\n","    epoch_loss = 0.0\n","\n","    # 학습률 조정\n","    current_lr = adjust_learning_rate(optimizer, epoch, initial_lr, num_epochs, warmup_epochs)\n","\n","    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n","        images_i, images_j = batch\n","        images_i, images_j = images_i.to(device), images_j.to(device)\n","\n","        # Forward pass\n","        features_q = mocov2_model(images_i)\n","\n","        # 모멘텀 인코더 업데이트\n","        mocov2_model._momentum_update()\n","\n","        with torch.no_grad():\n","            # 모멘텀 인코더를 사용하여 이미지를 인코딩하여 features_k를 계산\n","            features_k = mocov2_model.momentum_encoder(images_j)\n","            features_k = mocov2_model.momentum_projection_head(features_k)\n","\n","            # CPU에 큐 업데이트\n","            mocov2_model.enqueue_dequeue(features_k)\n","\n","            # 큐를 비동기적으로 GPU로 이동\n","            queue_gpu = mocov2_model.queue.to(device, non_blocking=True)\n","\n","        # Loss 계산 (비동기적으로 이동된 queue 사용)\n","        loss = criterion(features_q, features_k, queue_gpu)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    # Average loss for the epoch\n","    epoch_loss /= len(train_loader)\n","    train_losses.append(epoch_loss)\n","\n","    # Print and save loss\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Learning Rate: {current_lr:.6f}')\n","\n","    # Save the loss at each epoch to a CSV file\n","    with open('/content/drive/MyDrive/final/train_loss.csv', mode='a') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([epoch + 1, epoch_loss])\n","\n","    if (epoch + 1) % 10 == 0:\n","        plt.plot(train_losses, label='Train Loss')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.title('Training Loss Over Epochs')\n","        plt.legend()\n","        plt.show()\n","        plt.close()\n","\n","    early_stopping(epoch_loss, mocov2_model)\n","\n","    if check_memory_usage(90):  # 예를 들어 사용 가능한 RAM의 90%를 초과하면\n","        print(\"High memory usage detected. Stopping training.\")\n","        early_stopping(epoch_loss, mocov2_model)\n","        break\n","\n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        plt.plot(train_losses, label='Train Loss')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.title('Training Loss Over Epochs')\n","        plt.legend()\n","        plt.show()\n","        break"]},{"cell_type":"markdown","metadata":{"id":"QlQ54eNkNtzH"},"source":["## save loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcp5F6C3S5Nf"},"outputs":[],"source":["loss_data = {'epoch': [i+1 for i in range(len(train_losses))], 'loss': train_losses}\n","\n","# 데이터프레임 생성\n","df = pd.DataFrame(loss_data)\n","\n","target_path = '/content/drive/MyDrive/final'\n","\n","# 폴더 생성\n","os.makedirs(target_path, exist_ok=True)\n","\n","# CSV 파일로 저장\n","# csv_path = target_path + '/loss_data_MoCov2_EfficientNetB2_231215.csv'  # 저장할 경로 지정\n","csv_path = target_path + '/loss_data_MoCov2_EfficientNetB2_231215.csv'  # 저장할 경로 지정\n","df.to_csv(csv_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"iLU1wZkzNxgS"},"source":["## save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sH82iChE86IV"},"outputs":[],"source":["torch.save(mocov2_model.state_dict(), '/content/drive/MyDrive/final/mocov2_best_model_231215.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7NFoxIFeS5v"},"outputs":[],"source":["from google.colab import files\n","# files.download('/content/drive/MyDrive/final/mocov2_best_model_231215.pth')\n","torch.save(optimizer.state_dict(), 'optimizer_state.pth')\n","\n","os.makedirs(target_path, exist_ok=True)\n","_model = MoCov2Model(efficientnet_b2, projection_dim=projection_dim, temperature=temperature,\n","                           dropout_rate=dropout_rate, queue_size=queue_size, momentum=momentum).to(device)\n","_model.load_state_dict(torch.load('/content/drive/MyDrive/final/mocov2_best_model_231215.pth'))\n","torch.save(_model, target_path + '/mocov2_best_model_total_231215.pth')"]},{"cell_type":"markdown","metadata":{"id":"pvOIEKPTaA4W"},"source":["## load model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5825,"status":"ok","timestamp":1702709820758,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"gePlgntGaCZb","outputId":"2ff550bf-99ad-4dd8-b773-ff0abb5d52e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b2-8bb594d6.pth\n","100%|██████████| 35.1M/35.1M [00:00<00:00, 236MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b2\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset\n","from torchvision import models, datasets\n","import os\n","from tqdm.auto import tqdm\n","from PIL import Image, ImageFile\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from efficientnet_pytorch import EfficientNet\n","import csv\n","import pandas as pd\n","import copy\n","from google.colab import drive\n","import pickle\n","\n","# epoch 및 hyperparameter 정의\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","Image.MAX_IMAGE_PIXELS = None\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","drive.mount('/content/drive')\n","\n","# Define the MoCov2 model\n","class MoCov2Model(nn.Module):\n","    def __init__(self, base_encoder, projection_dim=128, temperature=0.07, dropout_rate=0.5, queue_size=384, momentum=0.99):\n","        super(MoCov2Model, self).__init__()\n","\n","        # 이미지 특징 추출 - 학습 & 추론 단계에서 모두 사용\n","        # EfficientNet + 128차원의 출력을 가진 선형 레이어 추가하여 DB에 저장할 특징 벡터의 차원 수 축소\n","        self.encoder = nn.Sequential(\n","            base_encoder,\n","            nn.Linear(1000, 128)\n","        )\n","\n","        # Encoder에 의해 추출된 특징을 projection - 학습 과정에서 contrastive loss를 계산하는 데 사용, 추론 시엔 사용 X\n","        # Projection head 추가\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(128, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, projection_dim),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        # 모멘텀 인코더의 복사본 생성 (깊은 복사)\n","        self.momentum_encoder = copy.deepcopy(self.encoder)\n","        self.momentum_projection_head = copy.deepcopy(self.projection_head)\n","\n","        # 모멘텀 인코더와 projection head의 모든 파라미터를 고정 (훈련 중 업데이트 방지)\n","        for param in self.momentum_encoder.parameters():\n","            param.requires_grad = False\n","        for param in self.momentum_projection_head.parameters():\n","            param.requires_grad = False\n","\n","        # 기타 초기화\n","        self.queue = torch.zeros(queue_size, projection_dim).to(device)\n","        self.queue_ptr = 0\n","        self.momentum = momentum\n","\n","    def _momentum_update(self):\n","        # 모멘텀 인코더 및 projection head의 가중치를 업데이트하는 함수\n","        with torch.no_grad():\n","            for param_q, param_k in zip(self.encoder.parameters(), self.momentum_encoder.parameters()):\n","                param_k.data = param_k.data * self.momentum + param_q.data * (1. - self.momentum)\n","            for param_q, param_k in zip(self.projection_head.parameters(), self.momentum_projection_head.parameters()):\n","                param_k.data = param_k.data * self.momentum + param_q.data * (1. - self.momentum)\n","\n","    def forward(self, x, with_projection_head=True):\n","        x = self.encoder(x)\n","        if with_projection_head:\n","            x = self.projection_head(x)\n","        return x\n","\n","    def enqueue_dequeue(self, keys):\n","        # 큐에 새로운 데이터 추가 및 오래된 데이터 제거 (CPU에서 수행)\n","        keys = keys.to('cpu')  # GPU에서 CPU로 이동\n","        batch_size = keys.size(0)\n","        ptr = int(self.queue_ptr)\n","\n","        # replace the keys at ptr (dequeue and enqueue)\n","        space_left = self.queue.size(0) - ptr  # 남은 공간 계산\n","        # 마지막 배치의 크기가 다른 배치보다 작을 때 발생할 수 있는 예외 상황 처리\n","        if space_left < batch_size:\n","            # If 큐의 남은 공간이 batch_size보다 작을 경우, split the update\n","            self.queue[ptr:] = keys[:space_left]\n","            self.queue[:batch_size - space_left] = keys[space_left:]\n","            ptr = batch_size - space_left\n","        else:\n","            self.queue[ptr:ptr + batch_size] = keys\n","            ptr = (ptr + batch_size) % self.queue.size(0)  # move pointer\n","\n","        self.queue_ptr = ptr\n","\n","# load mean & std\n","file_path = '/content/drive/MyDrive/final/dataset_mean_std.pkl'\n","\n","with open(file_path, 'rb') as f:\n","    mean_, std_ = pickle.load(f)\n","\n","efficientnet_b2 = EfficientNet.from_pretrained('efficientnet-b2').to(device)\n","mocov2_model = MoCov2Model(efficientnet_b2).to(device)\n","\n","mocov2_model.load_state_dict(torch.load('/content/drive/MyDrive/final/mocov2_best_model_231216.pth'))"]},{"cell_type":"markdown","metadata":{"id":"Yf4i_QmCPAb7"},"source":["# similarity test"]},{"cell_type":"markdown","metadata":{"id":"HwJCA_gXPIH8"},"source":["## save feature vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["fac7e84a3d084955abcfa6c6ec91217e","caeceae6618c4cd98a1d08eebc86e1cc","73d1a8af24074a54aa348ace2ed5da2c","7ba3ee908e3d49da8f9121d502d74c27","11fe5a2b241f4f1ca240ea7d6431bcd4","279b0877c6e749e08076c7c19272883e","f6778439d0dc46d9ad485f2e52dc1c39","812cb358b68b401085c26f7534f9885f","edf4816cd01e455db42acbcfbea272b4","b92b3dd68fdc43bd94ba8317de91b698","f96ff40df70c4eacb66b79ed805b1e44"]},"executionInfo":{"elapsed":1495244,"status":"ok","timestamp":1702712605714,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"NdUDAtMYOyfl","outputId":"0937f3c4-ffa2-468f-8c26-f9d3c1793973"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fac7e84a3d084955abcfa6c6ec91217e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/56787 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm.auto import tqdm\n","import os\n","import pickle\n","\n","# GPU 사용 가능 여부 확인\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean_, std_), # 학습 시 사용했던 평균 & 표준편차 값 사용\n","])\n","\n","# 이미지 파일 경로 리스트를 가져오는 함수\n","def get_image_paths(folder_path):\n","    image_files = os.listdir(folder_path)\n","    image_paths = [os.path.join(folder_path, img) for img in image_files if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","    return image_paths\n","\n","# 특징 벡터 dictionary를 pickle로 저장하는 함수\n","def save_feature_vectors(feature_vectors, file_path):\n","    with open(file_path, 'wb') as f:\n","        pickle.dump(feature_vectors, f)\n","\n","# dataset의 특징 벡터를 저장하는 함수\n","def save_features(model, dataset_path, test_transform, feature_vectors_path):\n","    model = model.to(device)  # 모델을 GPU로 이동\n","    model.eval()\n","    feature_vectors = {}\n","\n","    with torch.no_grad():\n","        for img_path in tqdm(get_image_paths(dataset_path)):\n","            dataset_image = test_transform(Image.open(img_path).convert('RGB')).unsqueeze(0).to(device)\n","            dataset_feature_vector = model(dataset_image, with_projection_head=False).cpu().numpy()\n","            feature_vectors[img_path] = dataset_feature_vector\n","\n","    save_feature_vectors(feature_vectors, feature_vectors_path)\n","\n","# 이미지들이 있는 폴더 및 특징 벡터 저장 파일 경로\n","dataset_path = '/content/Poster/Movie'\n","# feature_vectors_path = '/content/drive/MyDrive/final/movie_feature_vectors_mocov2_model.pkl'\n","feature_vectors_path = '/content/movie_feature_vectors_mocov2_model.pkl'\n","\n","# 특징 벡터 저장\n","save_features(mocov2_model, dataset_path, test_transform, feature_vectors_path)"]},{"cell_type":"markdown","metadata":{"id":"svzBPVHJPLAE"},"source":["## load feature vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzrRnEcKPQGc"},"outputs":[],"source":["# 특징 벡터를 load\n","\n","def load_feature_vectors(file_path):\n","    with open(file_path, 'rb') as f:\n","        feature_vectors = pickle.load(f)\n","    return feature_vectors\n","\n","feature_vectors_path = '/content/movie_feature_vectors_mocov2_model.pkl'\n","feature_vectors = load_feature_vectors(feature_vectors_path)"]},{"cell_type":"markdown","metadata":{"id":"xV0Bk5tVPM9U"},"source":["## calculate similarity 1 vs many"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RDwXneLPUts"},"outputs":[],"source":["# 이미지 유사도를 측정하는 함수\n","def measure_similarity(model, feature_vectors, test_image_path, test_transform):\n","    model = model.to(device)\n","    model.eval()\n","    similarities = []\n","\n","    # 테스트 이미지 로드 및 전처리\n","    test_image = test_transform(Image.open(test_image_path).convert('RGB')).unsqueeze(0).to(device)\n","\n","    # 모든 이미지에 대해 유사도 계산\n","    with torch.no_grad():\n","        test_feature_vector = model(test_image)\n","\n","        for img_path, feature_vector in feature_vectors.items():\n","            feature_vector = torch.tensor(feature_vector).to(device)\n","            similarity_score = nn.functional.cosine_similarity(test_feature_vector, feature_vector)\n","            similarities.append((img_path, similarity_score.item()))\n","\n","    return similarities\n","\n","# 유사도를 측정할 이미지 path\n","test_image_path = '/caseC3.jpg'  # 유사도를 측정할 대상 이미지 경로\n","\n","# 유사도 측정 실행\n","similarities = measure_similarity(mocov2_model, feature_vectors, test_image_path, test_transform)\n","\n","# 상위 top_cnt개 유사도\n","top_cnt = 20\n","\n","top_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_cnt]"]},{"cell_type":"markdown","metadata":{"id":"5KXpDiHKPTBc"},"source":["## result plot"]},{"cell_type":"markdown","metadata":{"id":"5o8zPt6TPYvY"},"source":["### without SSIM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"executionInfo":{"elapsed":10185,"status":"ok","timestamp":1702713888013,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"G7LtGrCVPUSE","outputId":"5c6159a6-49c1-455a-ca68-97688ef6f4de"},"outputs":[],"source":["from skimage.metrics import structural_similarity as ssim\n","import cv2\n","\n","# 상위 top_cnt개 이미지 플롯\n","fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n","fig.suptitle(f'Top 10 Similar Images')\n","\n","for i, (img_path, sim_score) in enumerate(top_similarities[:10]):\n","    ax = axes[i // 5, i % 5]\n","    image = Image.open(img_path)\n","    ax.imshow(image)\n","    ax.set_title(f\"cos_sim: {sim_score:.2f}\")\n","    ax.axis('off')  # 축 표시를 끕니다.\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"m63zdavsPa-p"},"source":["### with SSIM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452,"referenced_widgets":["04e6dd62e50b49e7982b14267ec7b7c2","be4c2a093502408399b725e0af3f6e21","8602b5cc83a24cd6b579b2e049beda07","3040e73f80b04ef099e600d622fe1247","5fc12af3dacd4946a12b9ee52baf7284","c6b31e2f53dd4ccfaad70ae504cb7b7f","3137f9581ace492da1d8f410fd81487b","924ed0fa40424d71b0f2f96e43fdff77","f0c012c907b8429e891680af1e6764cb","7ce6f09d24ae408191a21abff091234a","ef3eec2e5e2d4d0891b91960d5d9128c"]},"executionInfo":{"elapsed":11888,"status":"ok","timestamp":1702713871613,"user":{"displayName":"지능정보 SW아카데미3조","userId":"15514715369125012048"},"user_tz":-540},"id":"nzzzGOcsPbOB","outputId":"12b42867-7635-4ecf-845e-7b3942d0cbdd"},"outputs":[],"source":["from skimage.metrics import structural_similarity as ssim\n","import cv2\n","\n","def sort_SSIM(top_similarities, test_image_path):\n","    new_top_similarities = []\n","    test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    for dataset_img_path, cos_sim in tqdm(top_similarities):\n","        dataset_image = cv2.imread(dataset_img_path, cv2.IMREAD_GRAYSCALE)\n","        if dataset_image is None:\n","            continue\n","        # dataset_image의 크기를 test_image과 동일하게 조절\n","        dataset_image = cv2.resize(dataset_image, (test_image.shape[1], test_image.shape[0]))\n","        ssim_index = ssim(test_image, dataset_image)\n","        new_top_similarities.append((dataset_img_path, cos_sim, ssim_index))\n","\n","    return sorted(new_top_similarities, key=lambda x: x[1] + x[2], reverse=True)\n","\n","top10_similarities = sort_SSIM(top_similarities, test_image_path)[:10]\n","\n","# 상위 top_cnt개 이미지 플롯\n","fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n","fig.suptitle(f'Top 10 Similar Images')\n","\n","for i, (img_path, sim_score, ssim_score) in enumerate(top10_similarities):\n","    ax = axes[i // 5, i % 5]\n","    image = Image.open(img_path)\n","    ax.imshow(image)\n","    ax.set_title(f\"cos_sim: {sim_score:.2f}, SSIM:{ssim_score:.2f}\")\n","    ax.axis('off')  # 축 표시를 끕니다.\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02c08f63b86b441f9fb8253f5eb155fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04e6dd62e50b49e7982b14267ec7b7c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be4c2a093502408399b725e0af3f6e21","IPY_MODEL_8602b5cc83a24cd6b579b2e049beda07","IPY_MODEL_3040e73f80b04ef099e600d622fe1247"],"layout":"IPY_MODEL_5fc12af3dacd4946a12b9ee52baf7284"}},"10451da80eb0424c8194ebfaec4b089d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"11fe5a2b241f4f1ca240ea7d6431bcd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14c00c02187d49b28025486bd5bc0d25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"279b0877c6e749e08076c7c19272883e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3040e73f80b04ef099e600d622fe1247":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ce6f09d24ae408191a21abff091234a","placeholder":"​","style":"IPY_MODEL_ef3eec2e5e2d4d0891b91960d5d9128c","value":" 20/20 [00:03&lt;00:00,  6.39it/s]"}},"3137f9581ace492da1d8f410fd81487b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50c2f42459ca4ce39b8b2854756c3897":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59a031273ad648a1842b4265b77765cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fc12af3dacd4946a12b9ee52baf7284":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73d1a8af24074a54aa348ace2ed5da2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_812cb358b68b401085c26f7534f9885f","max":56787,"min":0,"orientation":"horizontal","style":"IPY_MODEL_edf4816cd01e455db42acbcfbea272b4","value":56787}},"7ba3ee908e3d49da8f9121d502d74c27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b92b3dd68fdc43bd94ba8317de91b698","placeholder":"​","style":"IPY_MODEL_f96ff40df70c4eacb66b79ed805b1e44","value":" 56787/56787 [24:55&lt;00:00, 44.51it/s]"}},"7ce6f09d24ae408191a21abff091234a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"812cb358b68b401085c26f7534f9885f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8602b5cc83a24cd6b579b2e049beda07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_924ed0fa40424d71b0f2f96e43fdff77","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0c012c907b8429e891680af1e6764cb","value":20}},"924ed0fa40424d71b0f2f96e43fdff77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a260deee41fa46038a3bcd4004958dd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff38d357f94046d896c0ce0c941bae8a","placeholder":"​","style":"IPY_MODEL_f98509c883b74804964a44b0db13faf5","value":" 275965/275965 [1:01:36&lt;00:00, 123.14it/s]"}},"aad1e6422bb74de8abf29b4b24b470e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbfe5b25047240b1a81163f5c4012b7c","IPY_MODEL_d26cd483ecfa43dbacc04cebeb661ded","IPY_MODEL_a260deee41fa46038a3bcd4004958dd1"],"layout":"IPY_MODEL_59a031273ad648a1842b4265b77765cd"}},"b92b3dd68fdc43bd94ba8317de91b698":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be4c2a093502408399b725e0af3f6e21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b31e2f53dd4ccfaad70ae504cb7b7f","placeholder":"​","style":"IPY_MODEL_3137f9581ace492da1d8f410fd81487b","value":"100%"}},"c6b31e2f53dd4ccfaad70ae504cb7b7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caeceae6618c4cd98a1d08eebc86e1cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_279b0877c6e749e08076c7c19272883e","placeholder":"​","style":"IPY_MODEL_f6778439d0dc46d9ad485f2e52dc1c39","value":"100%"}},"cbfe5b25047240b1a81163f5c4012b7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50c2f42459ca4ce39b8b2854756c3897","placeholder":"​","style":"IPY_MODEL_14c00c02187d49b28025486bd5bc0d25","value":"100%"}},"d26cd483ecfa43dbacc04cebeb661ded":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02c08f63b86b441f9fb8253f5eb155fa","max":275965,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10451da80eb0424c8194ebfaec4b089d","value":275965}},"edf4816cd01e455db42acbcfbea272b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef3eec2e5e2d4d0891b91960d5d9128c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0c012c907b8429e891680af1e6764cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6778439d0dc46d9ad485f2e52dc1c39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f96ff40df70c4eacb66b79ed805b1e44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f98509c883b74804964a44b0db13faf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fac7e84a3d084955abcfa6c6ec91217e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_caeceae6618c4cd98a1d08eebc86e1cc","IPY_MODEL_73d1a8af24074a54aa348ace2ed5da2c","IPY_MODEL_7ba3ee908e3d49da8f9121d502d74c27"],"layout":"IPY_MODEL_11fe5a2b241f4f1ca240ea7d6431bcd4"}},"ff38d357f94046d896c0ce0c941bae8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
